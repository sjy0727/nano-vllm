# ğŸš€ Nano-vLLM OpenAI Gateway

[![è®¸å¯è¯](https://img.shields.io/badge/license-Apache%202.0-blue.svg)](https://github.com/sjy0727/nano-vllm/blob/main/LICENSE)
[![Python](https://img.shields.io/badge/python-3.10+-green.svg)](https://www.python.org/)
[![FastAPI](https://img.shields.io/badge/FastAPI-powered-green.svg)](https://fastapi.tiangolo.com/)

ä¸€ä¸ªä¸º Nano-vLLM å¼•æ“æ‰“é€ çš„è½»é‡çº§ã€é«˜æ€§èƒ½ã€OpenAI å…¼å®¹çš„ API æœåŠ¡å™¨ã€‚

## âœ¨ åŠŸèƒ½ç‰¹æ€§

- ğŸ”„ **OpenAI API å…¼å®¹æ€§**: å®Œå…¨æ”¯æŒèŠå¤©å’Œæ–‡æœ¬è¡¥å…¨ã€‚
- ğŸŒŠ **æµå¼å“åº”**: é€šè¿‡ Server-Sent Events (SSE) å®ç°å®æ—¶ token æµã€‚
- âš¡ **é«˜æ€§èƒ½å¼•æ“**: ç”±ä¸€ä¸ªå®šåˆ¶çš„ç±» PagedAttention çš„ CUDA å¼•æ“é©±åŠ¨ã€‚
- âš™ï¸ **å¼ é‡å¹¶è¡Œ**: ä½¿ç”¨ `--tp` å‚æ•°åœ¨å¤šä¸ª GPU ä¸Šè¿è¡Œå¤§æ¨¡å‹ã€‚
- ğŸ“ **API æ–‡æ¡£**: åœ¨ `/docs` è·¯å¾„ä¸‹è‡ªåŠ¨ç”Ÿæˆäº¤äº’å¼ Swagger UIã€‚
- ğŸ”‘ **API å¯†é’¥å®‰å…¨**: å¯é€‰çš„ bearer token èº«ä»½éªŒè¯ã€‚
- ğŸ› ï¸ **å‡½æ•°è°ƒç”¨/å·¥å…·è°ƒç”¨**ï¼šæ”¯æŒ OpenAI é£æ ¼çš„ Function Callingï¼Œå¯åœ¨ chat completions è¯·æ±‚ä¸­ä¼ é€’ `tools` å’Œ `tool_choice`ï¼ŒæœåŠ¡å™¨ä¼šè‡ªåŠ¨è§£æå¹¶è¿”å›å·¥å…·è°ƒç”¨ï¼ˆæ”¯æŒæµå¼ï¼‰ã€‚
- ğŸ§© **JSON æ¨¡å¼**ï¼šæ”¯æŒ `response_format={"type": "json_object"}`ï¼Œå¼ºåˆ¶æ¨¡å‹è¾“å‡ºåˆæ³• JSON å¯¹è±¡ã€‚
- ğŸ“¦ **åŠ¨æ€æ¨¡å‹åˆ—è¡¨**ï¼š`/v1/models` åªè¿”å›å½“å‰å®é™…åŠ è½½çš„æ¨¡å‹ï¼Œå§‹ç»ˆä¸æœåŠ¡å™¨çŠ¶æ€ä¸€è‡´ã€‚
- ğŸ—ï¸ **å…¨å±€çŠ¶æ€é›†ä¸­ç®¡ç†**ï¼šæ‰€æœ‰æœåŠ¡ç«¯çŠ¶æ€ï¼ˆæ¨¡å‹ã€åˆ†è¯å™¨ã€é…ç½®ã€å¯†é’¥ï¼‰å‡é€šè¿‡ FastAPI çš„ `app.state` ç»Ÿä¸€ç®¡ç†ï¼Œä¾¿äºç»´æŠ¤å’Œæ‰©å±•ã€‚

## ğŸš€ å®‰è£…

æœ¬æœåŠ¡å™¨ç›´æ¥ä»æºä»£ç ä»“åº“è¿è¡Œã€‚

```bash
# å…‹éš†ä»“åº“
git clone https://github.com/sjy0727/nano-vllm
cd nano-vllm

# å®‰è£…ä¾èµ–
pip install -r requirements.txt 
# æˆ–å¦‚æœä½¿ç”¨ pyproject.toml
# pip install -e .
```

## ğŸ”§ å¿«é€Ÿå¼€å§‹

### 1. å¯åŠ¨æœåŠ¡å™¨

```bash
# ä½¿ç”¨é»˜è®¤è®¾ç½®å¯åŠ¨
python -m nanovllm.cli.server --model-path /path/to/your/model

# åœ¨æŒ‡å®šçš„ä¸»æœºå’Œç«¯å£ä¸Šä½¿ç”¨ 2 ä¸ª GPU è¿è¡Œ
python -m nanovllm.cli.server \
  --model-path /path/to/qwen3-0.6b \
  --host 0.0.0.0 \
  --port 8080 \
  --tp 2
```

### 2. API ç¤ºä¾‹

#### åˆ—å‡ºæ¨¡å‹

```bash
curl http://localhost:8000/v1/models
```

#### èŠå¤©è¡¥å…¨ (éæµå¼)

```bash
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "qwen3-0.6b",
    "messages": [{"role": "user", "content": "ä½ å¥½!"}],
    "stream": false
  }'
```

ç¤ºä¾‹å“åº”:
```json
{
  "id": "chatcmpl-...",
  "object": "chat.completion",
  "created": 1719123456,
  "model": "qwen3-0.6b",
  "choices": [{
    "index": 0,
    "message": {
      "role": "assistant",
      "content": "ä½ å¥½ï¼æˆ‘èƒ½ä¸ºä½ åšç‚¹ä»€ä¹ˆï¼Ÿ"
    },
    "finish_reason": "stop"
  }],
  "usage": {
    "prompt_tokens": 9,
    "completion_tokens": 9,
    "total_tokens": 18
  }
}
```

### 3. ä½¿ç”¨ Python å®¢æˆ·ç«¯

```python
from openai import OpenAI

client = OpenAI(
    base_url="http://localhost:8000/v1",
    api_key="dummy"  # å¿…éœ€ï¼Œä½†å¦‚æœæœåŠ¡å™¨æœªè®¾ç½®å¯†é’¥åˆ™ä¸ä¼šæ£€æŸ¥
)

# æµå¼è¯·æ±‚
for chunk in client.chat.completions.create(
    messages=[{"role": "user", "content": "ç»™æˆ‘è®²ä¸ªçŸ­æ•…äº‹ã€‚"}],
    model="qwen3-0.6b",
    stream=True
):
    print(chunk.choices[0].delta.content or "", end="")
```

## âš™ï¸ é…ç½®

é…ç½®é€šè¿‡å‘½ä»¤è¡Œå‚æ•°è¿›è¡Œç®¡ç†ã€‚

| å‚æ•° | ç¯å¢ƒå˜é‡ | æè¿° | é»˜è®¤å€¼ |
|---|---|---|---|
| `--host` | - | æœåŠ¡å™¨ä¸»æœº | `0.0.0.0` |
| `--port` | - | æœåŠ¡å™¨ç«¯å£ | `8000` |
| `--model-path`| `NANOVLLM_MODEL_PATH` | LLM æ¨¡å‹è·¯å¾„ | `~/llms/Qwen3-0.6B/` |
| `--api-key` | - | ç”¨äºä¿æŠ¤ API çš„å¯†é’¥ (å¯å¤šæ¬¡ä½¿ç”¨) | æ—  |
| `--tp` | - | å¼ é‡å¹¶è¡Œå¤§å° | `1` |

## ğŸ”Œ API ç«¯ç‚¹

æœåŠ¡å™¨æä¾›ä»¥ä¸‹ä¸»è¦ç«¯ç‚¹ï¼š

- `POST /v1/chat/completions`: ç”ŸæˆåŸºäºèŠå¤©çš„è¡¥å…¨ã€‚
- `POST /v1/completions`: ç”Ÿæˆæ ‡å‡†æ–‡æœ¬è¡¥å…¨ã€‚
- `GET /v1/models`: åˆ—å‡ºå¯ç”¨æ¨¡å‹ã€‚
- `GET /health`: æœåŠ¡å™¨å¥åº·æ£€æŸ¥ã€‚

### æ”¯æŒçš„å‚æ•°

`chat/completions` å’Œ `completions` è¯·æ±‚çš„å…³é”®å‚æ•°åŒ…æ‹¬ï¼š

| å‚æ•° | ç±»å‹ | é»˜è®¤å€¼ | æè¿° |
|---|---|---|---|
| `model` | string | **å¿…éœ€** | è¦ä½¿ç”¨çš„æ¨¡å‹ID |
| `messages`/`prompt` | array/string | **å¿…éœ€** | æ¨¡å‹çš„è¾“å…¥ |
| `stream` | boolean | `false` | æ˜¯å¦æµå¼å“åº” |
| `temperature` | float | `0.7` | é‡‡æ ·æ¸©åº¦ |
| `max_tokens` | integer | `2048` | ç”Ÿæˆçš„æœ€å¤§ token æ•° |
| `top_p` | float | `1.0` | æ ¸é‡‡æ ·å‚æ•° |
| `n` | integer | `1` | è¦ç”Ÿæˆçš„é€‰æ‹©æ•°é‡ |
| `presence_penalty`| float | `0.0` | å­˜åœ¨æƒ©ç½š |
| `frequency_penalty`| float | `0.0` | é¢‘ç‡æƒ©ç½š |

**æ³¨æ„**: `usage` å­—æ®µä»…åœ¨éæµå¼å“åº”ä¸­æä¾›ã€‚

## ğŸ› ï¸ å¼€å‘

```bash
# å…‹éš†ä»“åº“
git clone https://github.com/sjy0727/nano-vllm
cd nano-vllm

# è®¾ç½®è™šæ‹Ÿç¯å¢ƒ
python -m venv venv
source venv/bin/activate

# ä»¥å¯ç¼–è¾‘æ¨¡å¼å®‰è£…
pip install -e .
```

## ğŸš¨ é—®é¢˜æ’æŸ¥

- **Connection refused (è¿æ¥è¢«æ‹’ç»)**: æ£€æŸ¥ `--host` å’Œ `--port` å‚æ•°ï¼Œå¹¶æ£€æŸ¥é˜²ç«å¢™è®¾ç½®ã€‚
- **Model not found (æ¨¡å‹æœªæ‰¾åˆ°)**: ç¡®ä¿ `--model-path` æ­£ç¡®ï¼Œå¹¶ä¸”æ‚¨æœ‰æƒé™è¯»å–æ–‡ä»¶ã€‚
- **CUDA out of memory (æ˜¾å­˜ä¸è¶³)**: å°è¯•å‡å°æ‰¹é‡å¤§å°æˆ–ä½¿ç”¨æ›´å°çš„æ¨¡å‹ã€‚å¦‚æœä½¿ç”¨å¼ é‡å¹¶è¡Œï¼ˆ`--tp`ï¼‰ï¼Œè¯·ç¡®ä¿æ‚¨æœ‰è¶³å¤Ÿçš„ GPU å’Œæ˜¾å­˜ã€‚

## ğŸ¤ è´¡çŒ®

æ¬¢è¿å„ç§è´¡çŒ®ï¼è¯·éšæ—¶æäº¤ï¼š
- Bug æŠ¥å‘Š
- åŠŸèƒ½è¯·æ±‚
- Pull requests
- æ–‡æ¡£æ”¹è¿›

## âš–ï¸ è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨ Apache 2.0 è®¸å¯è¯ - è¯¦æƒ…è¯·å‚é˜… [LICENSE](LICENSE) æ–‡ä»¶ã€‚ 